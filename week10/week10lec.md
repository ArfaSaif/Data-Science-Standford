
<img width="1418" alt="image" src="https://user-images.githubusercontent.com/48233453/130545065-e898d549-667f-4e50-92f2-fa93038b632c.png">


- machine learning need to specify the features compared to deep learning you dont have to
- size of data in deep learning must be very great compared to machine learning
- 
<img width="1420" alt="image" src="https://user-images.githubusercontent.com/48233453/130545557-9be83e0e-71bd-4d81-a41a-b025f7192988.png">
<img width="1276" alt="image" src="https://user-images.githubusercontent.com/48233453/130545864-5a2cb625-febf-418a-9ae0-ad2b4a5de2d8.png">
- latent space


<img width="1430" alt="image" src="https://user-images.githubusercontent.com/48233453/130546179-dd90c199-f07d-4402-b8e0-a9d761e42eb1.png">
# the gaps in AI
1) deep learning is supervised
- data alot is required, labelled, need to move towards more unsupervised learning
-   distanglement learning - automatically extracting features from your dataset - a trend in the industry


2) not robust: in self driving cars - if draw a black box on a stop sign model will think its something else: Fix it using Recurrent Feedback , Uncertainty Quantification
3) need to make adaptive models, need lifelong learning models without the need the of retraining manually, they should be smart to learn and retrain themselves
- once you find some gaps or misclassification, need to retrain the model , but that can be too late in self-driving cars- before you do retraining of the model - 

<img width="1331" alt="image" src="https://user-images.githubusercontent.com/48233453/130546863-02f8d25b-1cc1-4073-842b-bdff071531d0.png">
babies:
- extract features on their own, by differentiating from adult and other babies


<img width="1434" alt="image" src="https://user-images.githubusercontent.com/48233453/130547231-53d31ce0-6cdd-4411-b6c0-959ff2fe2809.png">


0 in machine learning we preprocessed the image - to find pixel defects, while in deep learning we passed it directly to the model

- not every operations run on gpu and tpu, everything can run on cpu
- need to reimplement - develop alogirhtms to be compatible on gpu and tpu
- all packages are compatible with tpu, gpu, can train using these algorithms


- for deploying scripts, models and most deep learning applications, python is more powerful, since they are open source, both R and python have similar deep learning packages, like keras in R

# Open CV
- cv2 package for image analysis, 


# why choose machine learning vs deep learning
- hyperparameter tunings
- need labelled images
- deep learning models are black boxes - hard to explain
- not ready for productions - need to make improvements
- future of AI is unsuperives - train models in unsupervised fashion: will see deep learning models only single or few images , that means labelling isnt going to be an issue
- distanglement learning - 
- 
<img width="1388" alt="image" src="https://user-images.githubusercontent.com/48233453/130548455-38360cb2-8c91-45ee-8a7a-eefaeb760789.png">


statquest is a good channel
